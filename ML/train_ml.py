import json
import pandas as pd
import joblib
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from pathlib import Path
from collections import Counter

# ======================
# PATHS
# ======================
DATASET_PATH = Path("ML/ml_dataset.json")
MODEL_PATH   = Path("ML/ml_model.pkl")

print("üå≤ Entra√Ænement ML Tempo OPTIMIS√â (GradientBoosting + boost hivernal)")

# ======================
# LOAD DATASET
# ======================
if not DATASET_PATH.exists():
    raise SystemExit("‚ùå Dataset ML introuvable")

df = pd.read_json(DATASET_PATH)

print(f"üìä √âchantillons disponibles : {len(df)}")
print("üß± Colonnes dataset :", list(df.columns))

if len(df) < 200:
    raise SystemExit("‚ùå Dataset insuffisant pour entra√Æner un mod√®le fiable")

# ======================
# ANALYSE DISTRIBUTION
# ======================
color_counts = df["color"].value_counts()
print("\nüìà Distribution des couleurs dans le dataset :")
for color, count in color_counts.items():
    pct = count / len(df) * 100
    print(f"   {color}: {count} ({pct:.1f}%)")

# Distribution par mois
print("\nüìÖ Distribution par mois (hiver = nov-mars) :")
df["_month"] = pd.to_datetime(df["date"]).dt.month
for month in [11, 12, 1, 2, 3]:
    month_df = df[df["_month"] == month]
    if len(month_df) > 0:
        month_dist = month_df["color"].value_counts(normalize=True) * 100
        print(f"   Mois {month:02d}: bleu={month_dist.get('bleu', 0):.0f}% blanc={month_dist.get('blanc', 0):.0f}% rouge={month_dist.get('rouge', 0):.0f}%")

# ======================
# FEATURE ENGINEERING
# ======================
print("\nüõ†Ô∏è Construction des features")

df["date"] = pd.to_datetime(df["date"], errors="coerce")
df = df.dropna(subset=["date"])

# Features temporelles de base
df["weekday"] = df["date"].dt.weekday
df["month"]   = df["date"].dt.month
df["day_of_month"] = df["date"].dt.day

# Harmonisation des noms (compatibilit√©)
if "temperature" in df.columns and "temp" not in df.columns:
    df["temp"] = df["temperature"]
if "rteConsommation" in df.columns and "rte" not in df.columns:
    df["rte"] = df["rteConsommation"]

# Features suppl√©mentaires si manquantes
if "horizon" not in df.columns:
    df["horizon"] = 0

if "winterBleuRemaining" not in df.columns:
    df["winterBleuRemaining"] = df.get("remainingBleu", 300)

if "isWinter" not in df.columns:
    df["isWinter"] = df["month"].isin([11, 12, 1, 2, 3]).astype(int)

if "isWeekend" not in df.columns:
    df["isWeekend"] = df["weekday"].isin([5, 6]).astype(int)

# üîë NOUVELLES FEATURES OPTIMIS√âES
# Cat√©gorie de temp√©rature (crucial pour pr√©diction)
df["temp_cat"] = pd.cut(
    df["temp"], 
    bins=[-50, -5, 0, 5, 10, 15, 50],
    labels=[0, 1, 2, 3, 4, 5]  # 0=tr√®s froid, 5=doux
).astype(int)

# Mois hivernal pond√©r√© (janvier/f√©vrier = pic)
df["winter_intensity"] = df["month"].map({
    11: 2,  # Novembre - d√©but hiver
    12: 3,  # D√©cembre - hiver
    1: 4,   # Janvier - pic hiver
    2: 4,   # F√©vrier - pic hiver  
    3: 2,   # Mars - fin hiver
}).fillna(0).astype(int)

# Stress de fin de saison (quotas √† √©couler)
df["quota_pressure"] = (
    (43 - df.get("remainingBlanc", 43)) / 43 * 0.5 +
    (22 - df.get("remainingRouge", 22)) / 22 * 0.5
).clip(0, 1)

# ======================
# TARGET
# ======================
df["label"] = df["color"]

# ======================
# FEATURES OPTIMIS√âES
# ======================
FEATURES = [
    # M√©t√©o (CRUCIAL)
    "temp",
    "temp_cat",
    "coldDays",
    
    # √ânergie
    "rte",
    
    # Calendrier
    "weekday",
    "month",
    "day_of_month",
    "isWeekend",
    "isWinter",
    "winter_intensity",
    
    # Contexte Tempo
    "remainingBlanc",
    "remainingRouge",
    "winterBleuRemaining",
    "seasonDayIndex",
    "quota_pressure",
    
    # Horizon pr√©diction
    "horizon"
]

TARGET = "label"

# V√©rification colonnes
available_features = [f for f in FEATURES if f in df.columns]
missing_features = [f for f in FEATURES if f not in df.columns]

if missing_features:
    print(f"‚ö†Ô∏è Features manquantes (ignor√©es) : {missing_features}")

FEATURES = available_features
print(f"‚úÖ Features utilis√©es : {FEATURES}")

df[FEATURES] = df[FEATURES].fillna(0)

X = df[FEATURES]
y = df[TARGET]

# ======================
# LABEL ENCODER
# ======================
le = LabelEncoder()
y_enc = le.fit_transform(y)

classes = list(le.classes_)
print(f"\nüè∑Ô∏è Classes apprises : {classes}")

# ======================
# CLASS WEIGHTS OPTIMIS√âS POUR L'HIVER
# ======================
# En hiver, bleu est RARE (surtout semaine)
# On p√©nalise fortement les erreurs sur blanc/rouge

# Calculer les poids inverses √† la fr√©quence
freq = Counter(y)
total = sum(freq.values())

# Poids de base inversement proportionnels √† la fr√©quence
# + boost manuel pour blanc/rouge car plus importants √† pr√©dire
BASE_WEIGHTS = {
    "bleu": 1.0,
    "blanc": max(4.0, total / (3 * freq.get("blanc", 1))),  # Min 4x
    "rouge": max(8.0, total / (3 * freq.get("rouge", 1)))   # Min 8x
}

class_weight = {
    le.transform([c])[0]: BASE_WEIGHTS[c]
    for c in classes
}

print(f"‚öñÔ∏è Poids calcul√©s : {BASE_WEIGHTS}")

# ======================
# TRAIN GRADIENT BOOSTING (meilleur que RandomForest pour ce cas)
# ======================
print("\nüöÄ Entra√Ænement GradientBoosting")

model = GradientBoostingClassifier(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.1,
    min_samples_leaf=10,
    subsample=0.8,
    random_state=42
)

# Note: GradientBoosting n'a pas de class_weight natif
# On utilise sample_weight √† la place
sample_weights = [BASE_WEIGHTS[label] for label in y]

model.fit(X, y_enc, sample_weight=sample_weights)

# ======================
# VALIDATION CROIS√âE
# ======================
print("\nüìä Validation crois√©e (5-fold)...")
scores = cross_val_score(model, X, y_enc, cv=5, scoring='accuracy')
print(f"   Accuracy moyenne : {scores.mean():.2%} (+/- {scores.std()*2:.2%})")

# ======================
# FEATURE IMPORTANCE
# ======================
print("\nüîç Importance des features :")
importances = sorted(
    zip(FEATURES, model.feature_importances_),
    key=lambda x: x[1],
    reverse=True
)
for feat, imp in importances[:10]:
    print(f"   {feat}: {imp:.3f}")

# ======================
# SAVE MODEL
# ======================
bundle = {
    "model": model,
    "label_encoder": le,
    "features": FEATURES,
    "class_weights": BASE_WEIGHTS,
    "model_type": "GradientBoosting"
}

MODEL_PATH.parent.mkdir(exist_ok=True)
joblib.dump(bundle, MODEL_PATH)

size = MODEL_PATH.stat().st_size

print(f"\n‚úÖ Mod√®le GradientBoosting entra√Æn√©")
print(f"üì¶ Taille du mod√®le : {size:,} bytes")

if size < 10_000:
    raise SystemExit("‚ùå Mod√®le anormalement petit")

print("\nüéâ ML optimis√© ‚Äî Blanc/Rouge mieux pond√©r√©s pour l'hiver !")
